{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confirmed-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import syft as sy\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "from typing import List, Tuple\n",
    "from uuid import UUID\n",
    "from uuid import uuid4\n",
    "from torch.utils.data import SequentialSampler, RandomSampler, BatchSampler\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "straight-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "refined-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, worker_list=None, n_workers=2):\n",
    "\n",
    "    if worker_list is None:\n",
    "        worker_list = list(range(0, n_workers))\n",
    "            \n",
    "    #counter to create the index of different data samples\n",
    "    idx = 0 \n",
    "    \n",
    "    #dictionary to accomodate the split data\n",
    "    dic_single_datasets = {}\n",
    "    for worker in worker_list: \n",
    "        \"\"\"\n",
    "        Each value is a list of three elements, to accomodate, in order: \n",
    "        - data examples (as tensors)\n",
    "        - label\n",
    "        - index \n",
    "        \"\"\"\n",
    "        dic_single_datasets[worker] = [] \n",
    "\n",
    "    \"\"\"\n",
    "    Loop through the dataset to split the data and labels vertically across workers. \n",
    "    Splitting method from @abbas5253: https://github.com/abbas5253/SplitNN-for-Vertically-Partitioned-Data/blob/master/distribute_data.py\n",
    "    \"\"\"\n",
    "    label_list = []\n",
    "    index_list = []\n",
    "    index_list_UUID = []\n",
    "    for tensor, label in dataset: \n",
    "        height = tensor.shape[-1]//len(worker_list)\n",
    "        i = 0\n",
    "        uuid_idx = uuid4()\n",
    "        for worker in worker_list[:-1]: \n",
    "            dic_single_datasets[worker].append(tensor[:, :, height * i : height * (i + 1)])\n",
    "            i += 1\n",
    "            \n",
    "        #add the value of the last worker / split\n",
    "        dic_single_datasets[worker_list[-1]].append(tensor[:, :, height * (i) : ])\n",
    "        label_list.append(torch.Tensor([label]))\n",
    "        index_list_UUID.append(uuid_idx)\n",
    "        index_list.append(torch.Tensor([idx]))\n",
    "        \n",
    "        idx += 1\n",
    "        \n",
    "    return dic_single_datasets, label_list, index_list, index_list_UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "disabled-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "trainset = datasets.MNIST('mnist', download=True, train=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "identified-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_single_datasets, label_list, index_list, index_list_UUID = split_data(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "electronic-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = \"0th_owner\"\n",
    "pickle.dump(dic_single_datasets[0], open(file, 'wb'))\n",
    "\n",
    "file = \"1st_owner\"\n",
    "pickle.dump(dic_single_datasets[1], open(file, 'wb'))\n",
    "\n",
    "file = \"indexlist\"\n",
    "pickle.dump(index_list, open(file, 'wb'))\n",
    "\n",
    "file = \"indexlistUUID\"\n",
    "pickle.dump(index_list_UUID, open(file, 'wb'))\n",
    "\n",
    "file = \"labellist\"\n",
    "pickle.dump(label_list, open(file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "altered-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = pickle.load(open(file, 'rb'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-investigator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-annex",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
